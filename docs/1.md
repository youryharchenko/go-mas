Це саме той момент, коли проект переростає з "прототипу" в "фреймворк".

Щоб розв'язати проблему "хардкоду", нам потрібно застосувати патерн **Inversion of Control (IoC)**. Замість того, щоб агент знав про "лабіринт" і "координати X,Y", він повинен знати про абстрактний `Domain` і абстрактний `State`.

Давайте винесемо ці поняття в окремий пакет (наприклад, `mas/planning`). Це дозволить вам у майбутньому замінити Лабіринт на, скажімо, "Гру в п'ятнашки" або "Робота-пилососа", не змінюючи логіку самого агента.

Ось як ми можемо формалізувати ці поняття в Go за допомогою **Інтерфейсів** та **Дженериків**.

### 1. Формалізація абстракцій (`mas/planning/types.go`)

Створимо пакет, який описує "контракти" для будь-якої задачі планування.

```go
package planning

// State - це "зліпок" реальності в конкретний момент.
// Він має бути порівнюваним (comparable), щоб ми могли використовувати його як ключ у мапі visited.
type State interface {
	comparable // Go 1.18+ (дозволяє використовувати тип як ключ мапи)
	String() string
}

// Action - це дія, яку можна виконати (наприклад, "MoveUp", "GrabObject").
type Action string

// Domain (або Environment) - описує правила світу.
// Агент запитує у Domain: "Що я можу зробити тут?" і "Що станеться, якщо я зроблю це?".
type Domain[S State] interface {
	// Actions повертає список доступних дій для даного стану
	Actions(s S) []Action
	
	// Result повертає новий стан після виконання дії
	Result(s S, a Action) S
	
	// IsGoal перевіряє, чи досягнуто мети
	IsGoal(s S) bool
	
	// StepCost повертає вартість дії (для A* в майбутньому, поки повертаємо 1)
	StepCost(from S, action Action, to S) float64
}

// Policy (Стратегія) - це "мозок". Вона приймає історію/пам'ять і вирішує, куди йти.
type Policy[S State] interface {
	// Decide повертає наступну дію або порожній рядок, якщо дій немає
	Decide(current S, domain Domain[S], memory Memory[S]) Action
}

// Memory - абстракція пам'яті (Visited, Stack, etc.)
type Memory[S State] interface {
	Remember(s S)
	HasVisited(s S) bool
    // Інші методи для роботи зі стеком/чергою...
}

```

### 2. Реалізація для Лабіринту (`models/maze_domain.go`)

Тепер ми "натягуємо" ці абстракції на наш конкретний випадок. Зверніть увагу: тут немає логіки пошуку, тільки "фізика" світу.

```go
package models

import (
	"fmt"
	"github.com/youryharchenko/go-mas/mas/planning"
)

// --- 1. STATE (Стан) ---
// MazeState - конкретна реалізація State
type MazeState struct {
	X, Y int
}
func (s MazeState) String() string { return fmt.Sprintf("%d,%d", s.X, s.Y) }

// --- 2. DOMAIN (Правила гри) ---
type MazeDomain struct {
	Grid []string // Карта (ReadOnly для агента)
}

func (d *MazeDomain) Actions(s MazeState) []planning.Action {
	var actions []planning.Action
	// Перевіряємо 4 напрямки, чи немає там стіни '#'
	// (Логіка перевірки меж масиву та стін)
    // ...
	return actions
}

func (d *MazeDomain) Result(s MazeState, a planning.Action) MazeState {
	// Повертає нові координати залежно від дії "UP", "DOWN" і т.д.
	next := s
	switch a {
	case "UP": next.Y--
	case "DOWN": next.Y++
    // ...
	}
	return next
}

func (d *MazeDomain) IsGoal(s MazeState) bool {
	// Перевіряє, чи є в цій клітинці 'E'
	return d.Grid[s.Y][s.X] == 'E'
}

func (d *MazeDomain) StepCost(s1 MazeState, a planning.Action, s2 MazeState) float64 {
	return 1.0 // Кожен крок коштує 1
}

```

### 3. Реалізація Стратегії "DFS" (`ai/dfs.go`)

Тепер найцікавіше. Алгоритм DFS більше не знає про "Maze". Він знає тільки про `Domain` і `State`. Це **Generic AI**.

```go
package ai

import "github.com/youryharchenko/go-mas/mas/planning"

// DFSPolicy - реалізує інтерфейс Policy
type DFSPolicy[S planning.State] struct {
	stack []S // Внутрішня пам'ять для бектрекінгу
}

func (p *DFSPolicy[S]) Decide(current S, domain planning.Domain[S], mem planning.Memory[S]) planning.Action {
	// 1. Отримати можливі дії
	actions := domain.Actions(current)
	
	// 2. Відфільтрувати ті, де ми вже були (використовуючи Memory)
	var unknownActions []planning.Action
	for _, a := range actions {
		nextState := domain.Result(current, a)
		if !mem.HasVisited(nextState) {
			unknownActions = append(unknownActions, a)
		}
	}

	// 3. Логіка вибору (DFS)
	if len(unknownActions) > 0 {
		// Йдемо в перший ліпший невідомий
		action := unknownActions[0]
		// Запам'ятовуємо поточний стан у стек, щоб повернутися, якщо там глухий кут
        // (Тут трохи спрощено, зазвичай кладуть у стек розвилки)
		p.stack = append(p.stack, current) 
		return action
	} else {
		// 4. Глухий кут? Backtracking!
		// Беремо зі стеку попередній стан і шукаємо шлях назад
        // (Це вимагає трохи складнішої логіки повернення, але суть зрозуміла)
		return "BACKTRACK" 
	}
}

```

### 4. Як тепер виглядає Агент?

Ваш `PlannerWalker` стає чистим виконавцем. Він просто з'єднує компоненти.

```go
type PlannerWalker struct {
	mas.BaseAgent
	
	// Компоненти архітектури
	Brain   planning.Policy[models.MazeState]  // Стратегія (DFS, Random, A*)
	World   planning.Domain[models.MazeState]  // Знання про фізику
	Memory  planning.Memory[models.MazeState]  // Пам'ять (Visited)
	
	CurrentState models.MazeState
}

func (a *PlannerWalker) Tick() {
    // 1. OODA Loop: Decide
    action := a.Brain.Decide(a.CurrentState, a.World, a.Memory)
    
    // 2. Act
    if action != "" {
        newState := a.World.Result(a.CurrentState, action)
        a.CurrentState = newState
        a.Memory.Remember(newState)
        // ... оновлення UI ...
    }
}

```

### Чому це краще?

1. **Замінність (Polymorphism):** Хочете змінити алгоритм на **A***? Просто замініть `DFSPolicy` на `AStarPolicy` при ініціалізації агента. Код агента не зміниться.
2. **Масштабованість:** Хочете зробити симуляцію "Торгівля на біржі"? Ви реалізуєте `Domain` для біржі (дії: `Buy`, `Sell`), а `DFSPolicy` (або краще `Q-Learning`) залишиться тим самим. Він просто шукатиме шлях у графі станів біржі.
3. **Тестування:** Ви можете тестувати логіку `DFS` на простих графах без запуску графічного інтерфейсу чи всієї MAS-системи.

### Наступний крок

Спробуйте створити теку `mas/planning` і перенести туди інтерфейси зі **Step 1**. Це фундамент вашої "мови Planner".